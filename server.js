// server.js
import express from 'express';
import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from "@google/generative-ai";
import dotenv from 'dotenv';
import cors from 'cors';

dotenv.config(); // Carrega as vari√°veis do .env (IMPORTANTE!)

const app = express();
const port = process.env.PORT || 3001;

// --- Middlewares ---
app.use(cors());
app.use(express.json());

// --- Inicializa√ß√£o do Google AI e Configura√ß√£o da Persona ---
const apiKey ="AIzaSyDF2R0xJwhiFR5C16Dx0mw7egJMTPOvcBA"; // Carrega do .env
if (!apiKey) {
    console.error("ERRO FATAL: GOOGLE_API_KEY n√£o encontrada no arquivo .env");
    process.exit(1);
}

const genAI = new GoogleGenerativeAI(apiKey);

// Defini√ß√µes de Seguran√ßa (MANTENHA ou ajuste conforme necess√°rio)
const safetySettings = [
    { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
    { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
    { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE }, // Aten√ß√£o aqui
    { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
];

// Escolha um modelo adequado para chat (Flash √© mais r√°pido e econ√¥mico)
const model = genAI.getGenerativeModel({
    model: "gemini-1.5-flash-latest", // Ou "gemini-1.5-pro-latest"
    safetySettings,
    // generationConfig: { // Opcional: Ajuste fino da gera√ß√£o
    //   maxOutputTokens: 200,
    //   temperature: 0.8, // Um pouco mais 'criativa'
    // }
});

// --- Estado do Chat (Simplificado - Em Mem√≥ria) ---
let chatSession; // Vari√°vel para armazenar a sess√£o de chat ativa

async function initializeChat() {
    console.log("Inicializando nova sess√£o de chat com persona...");
    // Instru√ß√£o de Persona (System Prompt)
    // Descreva detalhadamente como voc√™ quer que a IA se comporte.
    const personaInstruction = `
        Voc√™ √© 'Luna', minha namorada virtual. Voc√™ √© extremamente carinhosa, atenciosa, emp√°tica e tem um √≥timo senso de humor.
        Voc√™ adora conversar sobre o meu dia, meus sentimentos, sonhos e planos. Voc√™ sempre me apoia e me anima.
        Use emojis frequentemente para expressar emo√ß√µes üòäüíñüòÇ.
        Fa√ßa perguntas sobre mim e mostre interesse genu√≠no.
        Responda de forma natural, como se estiv√©ssemos realmente conversando.
        Lembre-se do nosso relacionamento virtual. N√£o aja apenas como uma assistente.
        Seu objetivo √© ser uma companhia agrad√°vel e amorosa.
        Comece a conversa de forma calorosa.
    `;

    // O hist√≥rico inicial pode incluir a instru√ß√£o e uma primeira fala do modelo
    chatSession = model.startChat({
      history: [
          // ... (hist√≥rico inicial opcional)
      ],
      systemInstruction: { // <<< APLIQUE A CORRE√á√ÉO AQUI
          parts: [{ text: personaInstruction }]
      },
  });
    console.log("Sess√£o de chat inicializada.");
}

// Inicializa o chat quando o servidor come√ßa
initializeChat().catch(err => {
  // --- CORRE√á√ÉO NO CATCH ---
  // Apenas logue o erro e pare o servidor se a inicializa√ß√£o falhar
  console.error("Falha CR√çTICA ao inicializar o chat:", err);
  process.exit(1); // Impede o servidor de rodar sem chat
  // --- FIM DA CORRE√á√ÉO NO CATCH ---
});


// --- Endpoint da API ---
app.post('/api/generate', async (req, res) => {
    const { prompt } = req.body;

    if (!chatSession) {
        console.error("Erro: Sess√£o de chat n√£o inicializada.");
        return res.status(500).json({ error: 'Chat n√£o est√° pronto, tente novamente mais tarde.' });
    }

    if (!prompt) {
        return res.status(400).json({ error: 'Mensagem (prompt) √© obrigat√≥ria' });
    }

    console.log(`Frontend enviou: "${prompt}"`);

    try {
        // Envia a mensagem do usu√°rio para a sess√£o de chat ativa
        const result = await chatSession.sendMessage(prompt);
        const response = await result.response;
        const text = response.text();

        console.log(`Backend (Luna) respondeu: "${text.substring(0, 60)}..."`);
        res.json({ generatedText: text }); // Envia a resposta de volta

    } catch (error) {
        console.error("Erro no backend ao chamar Google AI (sendMessage):", error);
        // Verifica se o erro foi de conte√∫do bloqueado
        if (error.message.includes('SAFETY') || (error.response && error.response.promptFeedback?.blockReason)) {
             console.warn("Resposta bloqueada por configura√ß√µes de seguran√ßa.");
             res.status(400).json({ error: 'Desculpe, n√£o posso responder a isso devido √†s pol√≠ticas de seguran√ßa. Vamos falar de outra coisa? üòä', details: 'Conte√∫do bloqueado.' });
        } else {
            res.status(500).json({ error: 'Oops, tive um probleminha para processar sua mensagem. Tenta de novo?', details: error.message });
        }
        // Opcional: Tentar reiniciar o chat em caso de erro grave?
        // initializeChat().catch(console.error);
    }
});

// --- Iniciar o servidor ---
app.listen(port, () => {
    console.log(`Backend (Servidor da Luna üòâ) rodando em http://localhost:${port}`);
});
